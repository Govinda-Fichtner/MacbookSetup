# MacbookSetup Project - Cursor Rules

## 🎯 Core Development Philosophy
**Minimal Complexity Principle**: Always seek the least amount of code and complexity to achieve the goal.

## 📋 Change Implementation Methodology

### 1. **Step-by-Step Validation Protocol**
- Make ONE small change at a time
- Run `zsh -n setup.sh` and `zsh -n verify_setup.sh` after each change
- Test execution with safe commands before proceeding
- Never make multiple unrelated changes in one step

### 2. **Pattern Reuse Strategy**
- ALWAYS look for existing patterns before creating new ones
- Reuse tree structure patterns from `generate_completion_files`
- Maintain consistency between `setup.sh` and `verify_setup.sh`
- Extend existing functions rather than creating new ones

### 3. **Testing Pipeline (Required Before Commit)**
```bash
# Required checks in this exact order:
1. zsh -n setup.sh && zsh -n verify_setup.sh
2. ./verify_setup.sh > /dev/null  # execution test
3. shellspec spec/ --format documentation  # ALL TESTS MUST PASS
4. pre-commit run --all-files
5. git add . && git commit -m "descriptive message"
6. git push origin main
```

### **🚫 NEVER COMMIT WITH FAILING TESTS**
**ABSOLUTE RULE**: NO commits are allowed until ALL tests pass completely.

#### **Test-First Development Protocol**
- ❌ **NEVER commit code with any failing tests** - even "minor" ones
- ❌ **NEVER skip test fixes** because "functionality works"
- ❌ **NEVER defer test fixes** to "next iteration"
- ✅ **ALWAYS run full test suite** before any commit: `shellspec spec/`
- ✅ **ALWAYS fix all failures** before proceeding to next development phase
- ✅ **ALWAYS get to 0 failures** - partial fixes are not acceptable

#### **Test Failure Resolution Strategy**
When tests fail:
1. **🛑 STOP development immediately** - no new features until tests pass
2. **🔍 Analyze each failure systematically** - understand root causes
3. **🔧 Fix one test at a time** - validate fix before moving to next
4. **🧪 Re-run full suite** after each fix to prevent regressions
5. **✅ Only proceed** when ALL tests show 0 failures

#### **Emergency Exception Protocol** (Extremely Rare)
If critical production issue requires bypassing:
1. **📋 Document specific reason** in commit message
2. **⏰ Set immediate timeline** for fixing tests (within hours, not days)
3. **🔄 Create follow-up task** with highest priority
4. **👥 Get explicit approval** from project owner
5. **🎯 Address test failures** before any other work

**Philosophy**: Tests are the safety net. Bypassing them creates technical debt and hidden bugs.

### **🚫 NEVER BYPASS PRE-COMMIT CHECKS**
**MANDATORY RULE**: Pre-commit hooks exist for quality control and MUST NEVER be bypassed.

#### **Forbidden Commands**
- ❌ **NEVER use `git commit --no-verify`** - This bypasses all validation
- ❌ **NEVER use `git commit -n`** - Short form of --no-verify
- ❌ **NEVER skip pre-commit fixes** because "functionality works"
- ❌ **NEVER commit with known linting/testing failures**

#### **Required Approach When Pre-commit Fails**
1. **🔧 FIX THE ISSUES** - Address shellcheck warnings, test failures, formatting
2. **📋 UNDERSTAND WHY** - Learn what the tools are catching
3. **✅ VERIFY FIXES** - Re-run pre-commit until it passes
4. **📝 UPDATE RULES** - Document new patterns if needed
5. **🎯 COMMIT ONLY AFTER ALL CHECKS PASS**

#### **Emergency Exception Protocol** (Rare Cases Only)
If there's a critical production issue requiring immediate bypass:
1. **📋 Document the reason** in commit message
2. **⏰ Set timeline** for fixing the bypassed issues
3. **🔄 Create follow-up task** to address the problems
4. **👥 Get approval** from team/project owner before bypassing

**Philosophy**: Pre-commit hooks catch issues before they become problems. Bypassing them creates technical debt and reduces code quality.

## ⚡ Simple Continuous Testing (SCT)

### **🎯 Core Principle**
**Fast Feedback Loop**: Run quick tests often, comprehensive tests periodically. Keep moving forward with small, tested changes.

### **📊 Two-Tier Testing (Simple)**

#### **🚀 Fast Tests (< 10 seconds)**
```bash
# Quick validation - run this often
sct-fast() {
  echo "⚡ Fast tests..."
  zsh -n setup.sh && zsh -n verify_setup.sh                    # Syntax check
  shellspec spec/mcp_manager_spec.sh --format progress         # Core functionality
}
```

#### **🔍 Full Tests (30+ seconds)**
```bash
# Comprehensive validation - run this periodically
sct-full() {
  echo "🔍 Full test suite..."
  shellspec spec/ --format documentation                       # All tests
  ./verify_setup.sh > /dev/null                               # Integration
  pre-commit run --all-files                                  # Quality checks
}
```

### **🔄 Simple Workflow**

#### **The 5-Change Rule**
```bash
# Track changes since last full test
SCT_CHANGE_COUNT_FILE=".sct_changes"

sct-commit() {
  # 1. Run fast tests
  sct-fast || { echo "❌ Fast tests failed - fix before committing"; return 1; }

  # 2. Commit the change
  git add .
  git commit -m "${1:-feat: incremental change}"

  # 3. Track change count
  local count
  count=$(cat "$SCT_CHANGE_COUNT_FILE" 2>/dev/null || echo 0)
  count=$((count + 1))
  echo "$count" > "$SCT_CHANGE_COUNT_FILE"

  # 4. Run full tests every 5 changes
  if [[ $count -ge 5 ]]; then
    echo "📊 5 changes reached - running full test suite..."
    if sct-full; then
      echo "✅ Full tests passed - resetting counter"
      echo "0" > "$SCT_CHANGE_COUNT_FILE"
    else
      echo "❌ Full tests failed - fix before continuing"
      return 1
    fi
  else
    echo "✅ Change $count/5 committed (full tests at 5)"
  fi
}
```

#### **Development Cycle**
```bash
# Simple development workflow
sct-dev() {
  echo "🔄 Simple Continuous Testing Development Cycle"
  echo ""
  echo "1. Make small change (single function/feature)"
  echo "2. Run: sct-commit 'description of change'"
  echo "3. Repeat"
  echo ""
  echo "Fast tests run every commit, full tests every 5 commits"
}
```

### **⏰ Time-Based Backup**
```bash
# Run full tests if it's been too long (15 minutes)
sct-check-time() {
  local last_test current_time diff
  last_test=$(stat -f %m "$SCT_CHANGE_COUNT_FILE" 2>/dev/null || echo 0)
  current_time=$(date +%s)
  diff=$((current_time - last_test))

  # 900 seconds = 15 minutes
  if [[ $diff -gt 900 ]]; then
    echo "⏰ 15+ minutes since last test - running full suite..."
    sct-full && touch "$SCT_CHANGE_COUNT_FILE"
  fi
}
```

### **🛠️ Setup (One-Time)**
```bash
# Initialize SCT
sct-setup() {
  echo "0" > .sct_changes
  echo "✅ Simple Continuous Testing initialized"
  echo ""
  echo "Usage:"
  echo "  sct-commit 'your change description'  # Fast test + commit"
  echo "  sct-fast                              # Run fast tests only"
  echo "  sct-full                              # Run full test suite"
  echo "  sct-check-time                        # Check if full tests needed"
}
```

### **📋 SCT Rules (Simple)**

#### **✅ Do This**
- **Make small changes** - one function, one feature, one fix at a time
- **Use `sct-commit`** instead of regular `git commit`
- **Fix fast test failures immediately** - don't accumulate broken tests
- **Run `sct-check-time`** when starting work sessions
- **Keep changes under 50 lines** when possible

#### **❌ Don't Do This**
- **Skip fast tests** because change seems "trivial"
- **Batch multiple unrelated changes** in one commit
- **Ignore full test failures** when they trigger
- **Work for hours without committing** anything

### **🎯 Value Delivered (80/20)**

#### **Core Benefits Achieved**
- ✅ **Fast feedback** on every change (< 10 seconds)
- ✅ **Prevents test debt accumulation** through 5-change rule
- ✅ **Maintains stability** with regular comprehensive testing
- ✅ **Simple workflow** that's easy to remember and follow
- ✅ **Small commits** that are easy to debug and revert

#### **Complexity Avoided**
- ❌ No complex change size detection algorithms
- ❌ No sophisticated scheduling systems
- ❌ No metrics tracking and reporting overhead
- ❌ No multiple test tier classifications
- ❌ No background processes or complex hooks

### **📝 Integration with Existing Rules**

#### **Enhanced Testing Pipeline**
```bash
# Updated simple pipeline:
1. sct-commit "description"          # Fast tests + commit (replaces manual process)
2. (Every 5th commit: full tests)    # Automatic comprehensive validation
3. git push origin main              # Push when ready

# For major changes, still use full pipeline:
1. sct-full                          # Full validation
2. git add . && git commit           # Manual commit for complex changes
3. git push origin main
```

#### **Aliases for Convenience**
```bash
# Add to .zshrc for easy access
alias sc='sct-commit'                # Quick commit with fast tests
alias sf='sct-fast'                  # Fast tests only
alias sF='sct-full'                  # Full test suite
alias st='sct-check-time'            # Time check
```

---

**Philosophy**: *Small changes + Fast tests + Simple rules = Steady progress with confidence*

### 4. **Visual Consistency Rules**
- Use tree structure (`├──`, `└──`) for hierarchical output
- Maintain color coding: `$GREEN` for success, `$YELLOW` for warnings, `$BLUE` for info
- Group related operations under section headers: `=== Section Name ===`
- Keep consistent indentation patterns across both scripts

## 🔧 Code Modification Guidelines

### **Output Formatting Standards**
```bash
# Section headers
echo -e "\n=== Section Name ==="

# Tree items
printf "├── %bOperation%b description\n" "$BLUE" "$NC"
printf "└── %b[SUCCESS]%b Operation completed\n" "$GREEN" "$NC"

# Nested items
printf "│   └── %b[SUCCESS]%b Sub-operation ready\n" "$GREEN" "$NC"
printf "    └── %b[SUCCESS]%b Deeply nested item\n" "$GREEN" "$NC"
```

### **Error Handling Preservation**
- Never remove existing error handling while making cosmetic changes
- Preserve all `|| return 1` and `|| exit 1` patterns
- Maintain graceful degradation for CI environments
- Keep warning messages for non-critical failures

### **Function Modification Rules**
- Prefer modifying output statements over changing function logic
- Use `printf` over `echo` for colored output with variables
- Replace `log_info` with section headers only for major groupings
- Keep `log_error` and `log_warning` for actual error conditions

## 📝 Commit Message Standards

### **Format**: `<type>: <description> - <details>`

**Types**: `feat`, `fix`, `refactor`, `style`, `docs`, `test`, `chore`

**Example**:
```
feat: add tree structure to setup.sh output for consistency
- Apply hierarchical tree output to match verify_setup.sh visual style
- Group operations under sections: Installing Packages, Language Environments
- Use minimal code changes, reusing existing tree patterns
- Maintain clean visual flow throughout setup process
```

## 🔧 Shell Completion Update Requirement

### **Mandatory Completion Check**
When modifying any shell script (*.sh files) OR reorganizing project structure, you MUST check if shell completion needs updates:

1. **Check completion file**: `support/completions/_mcp_manager` for mcp_manager.sh changes
2. **Verify commands match**: Ensure completion options match actual commands
3. **Test completion**: Verify tab completion works for new/modified commands
4. **Update if needed**: Add new commands, options, or server IDs to completion
5. **Validate symlinks**: Check that ~/.zsh/completions/ symlinks point to correct locations

### **Files to Check**:
- `support/completions/_mcp_manager` - Completion for mcp_manager.sh
- `~/.zsh/completions/_mcp_manager` - User's completion symlink (if exists)
- Any other completion files in the project

### **CRITICAL: New MCP Server Addition Protocol**
When adding new MCP servers (like docker, slack, etc.), you MUST update shell completion:

1. **Update server list in completion**: Add new server_id to completion options
2. **Test tab completion**: Verify `./mcp_manager.sh test <TAB>` shows new server
3. **Update all command contexts**: test, setup, config, inspect commands should all recognize new server
4. **Validate completion works**: Test in clean shell: `autoload -U compinit && compinit`

**Example**: After adding "docker" server, completion should show:
```bash
./mcp_manager.sh test <TAB>  # Should show: github circleci filesystem docker
./mcp_manager.sh setup <TAB> # Should show: github circleci filesystem docker
```

### **Testing Protocol**:
```bash
# 1. Verify completion file exists and is readable
test -r support/completions/_mcp_manager && echo "✅ Completion file readable" || echo "❌ File missing/unreadable"

# 2. Check symlink validity (if user has completion installed)
if [[ -L ~/.zsh/completions/_mcp_manager ]]; then
  test -r ~/.zsh/completions/_mcp_manager && echo "✅ Symlink valid" || echo "❌ Broken symlink - needs update"
fi

# 3. Test completion functionality in clean shell
autoload -U compinit && compinit
./mcp_manager.sh <TAB><TAB>  # Should show available commands
```

### **File Relocation Protocol**
When moving completion files:
- ✅ **Update all references** in documentation and setup scripts
- ✅ **Check for existing user symlinks** in ~/.zsh/completions/
- ✅ **Provide migration guidance** or automatic symlink updates
- ✅ **Test completion works** after relocation

## 🚫 Anti-Patterns to Avoid

### **Complexity Anti-Patterns**
- ❌ Creating new utility functions when existing ones work
- ❌ Adding configuration files for simple formatting changes
- ❌ Over-engineering solutions that could be simple printf statements
- ❌ Breaking working functionality for cosmetic improvements

### **Testing Anti-Patterns**
- ❌ Making multiple changes without intermediate testing
- ❌ Committing without running the full testing pipeline
- ❌ Assuming syntax check means execution will work
- ❌ Skipping pre-commit hooks "just this once"

### **Security Anti-Patterns**
- ❌ **NEVER commit `.envrc` files** - They contain sensitive environment variables
- ❌ **NEVER commit backup files** - `.env.backup`, `*.bak` may contain sensitive tokens
- ❌ Committing any file with real API tokens, passwords, or secrets
- ❌ Using real credentials in example files (use placeholders like "your-token-here")
- ❌ Ignoring GitHub push protection warnings about detected secrets

## 🎨 Project-Specific Patterns

### **Shell Script Consistency**
- Both `setup.sh` and `verify_setup.sh` should have matching visual output
- Use shared color variables: `$RED`, `$GREEN`, `$BLUE`, `$YELLOW`, `$NC`
- Extract version numbers using the shared `extract_version()` function
- Handle missing files gracefully with warnings, not errors

### **CI/Local Environment Handling**
- Always check `[[ "${CI:-false}" == "true" ]]` for CI-specific behavior
- Use `SKIP_ORBSTACK=true` for testing without Docker dependencies
- Provide graceful fallbacks for missing dependencies in CI

### **Zsh Compatibility**
- Use `for item in "${array[@]}"` instead of bash-style `${!array[@]}`
- Test all changes with `zsh -n` syntax checking
- Ensure compatibility with both zsh and bash environments

## 🛠️ Tool Integration Patterns (Added from Node.js/nvm experience)

### **Tool Classification and Handling**
- **Standard CLI Tools**: rbenv, pyenv, terraform → Direct command line access
- **Sourced Tools**: nvm → Require shell sourcing for initialization
- **GUI Applications**: Warp, iTerm2 → Configuration via plist manipulation
- **Container Tools**: Docker, OrbStack → Environment-dependent availability

### **Language Environment Standards**
```bash
# Template for adding new language environments:
setup_<language>_environment() {
  printf "├── %b<Language> Environment%b\n" "$BLUE" "$NC"

  # 1. Validate tool availability (with proper sourcing if needed)
  # 2. Determine latest version using tool-specific method
  # 3. Check if version already installed (tool-specific detection)
  # 4. Background installation with progress spinner
  # 5. Set default/global version
  # 6. Optional: Update package manager
  # 7. Success confirmation
}
```

### **Verification Complexity Handling**
- **Standard Tools**: Use `check_command "$tool"` and `"$tool" --version`
- **Sourced Tools**: Create special case blocks with proper initialization
- **Version Extraction**: Add tool-specific patterns to `extract_version()` function
- **Environment Graceful Degradation**: Distinguish between missing vs unavailable in CI

### **Shell Integration Rules**
- Tools requiring sourcing (like nvm) need:
  - Initialization in setup functions: `source "$(brew --prefix)/opt/tool/tool.sh"`
  - Special verification logic: Custom detection blocks in verify_setup.sh
  - Shell config addition: Proper PATH and sourcing in .zshrc
  - Completion handling: Both shell integration AND completion files

### **Error Handling for Special Cases**
- Use tool-specific error detection rather than generic patterns
- Provide informative error messages about tool requirements
- Maintain CI compatibility with appropriate warnings vs errors
- Always test both "tool not installed" and "tool installed but not working" scenarios

### **Consistency Enforcement Rules**
- Language environments must follow identical tree structure patterns
- All background installations must use `show_progress()` with proper spinner
- Version extraction must use shared `extract_version()` function
- Verification logic must handle tool-specific requirements while maintaining output consistency

## 🔗 MCP Server Integration Patterns (Added from GitHub/CircleCI experience)

### **MCP Server Addition Protocol**
When adding new MCP servers to the project, follow this exact pattern:

```bash
# 1. Add server to AVAILABLE_SERVERS array in mcp_manager.sh
AVAILABLE_SERVERS=("github" "circleci" "new-server")

# 2. Add Docker image mapping in get_docker_image()
get_docker_image() {
  case "$1" in
    "github") echo "mcp/github-mcp-server:latest" ;;
    "circleci") echo "local/mcp-server-circleci:latest" ;;
    "new-server") echo "vendor/new-server:latest" ;;
    *) return 1 ;;
  esac
}

# 3. Add environment variables in get_expected_env_vars()
get_expected_env_vars() {
  case "$1" in
    "new-server")
      echo "NEW_SERVER_TOKEN NEW_SERVER_API_KEY NEW_SERVER_BASE_URL"
      ;;
  esac
}

# 4. Add placeholder mapping in get_env_placeholder()
get_env_placeholder() {
  case "$1" in
    "NEW_SERVER_TOKEN") echo "your_new_server_token_here" ;;
    "NEW_SERVER_API_KEY") echo "your_api_key_here" ;;
    "NEW_SERVER_BASE_URL") echo "https://api.newserver.com" ;;
  esac
}
```

### **Configuration Generation Standards**
- **ALWAYS use --env-file approach**: Never inline environment variables in JSON
- **Use absolute paths**: `--env-file /Users/gfichtner/MacbookSetup/.env`
- **Generate .env_example**: Never overwrite existing `.env` files
- **Include all available servers**: Use `get_available_servers()` not just working ones
- **Maintain JSON structure consistency**: Both Cursor and Claude Desktop formats

### **Environment File Safety Rules**
- ❌ **NEVER generate or overwrite .env files directly**
- ✅ **ALWAYS generate .env_example with placeholders**
- ✅ **Read tokens from .env file, not environment variables**
- ✅ **Validate against placeholders**: Detect "your_token_here" patterns
- ✅ **Preserve existing .env**: Show "[INFO] Existing .env file found (keeping as-is)"

### **Mount-Based Server Configuration (Filesystem Example)**
**Key Pattern for Directory-Based MCP Servers**:

- **Multiple Directory Support**: Use comma-separated paths in .env_example
```bash
FILESYSTEM_ALLOWED_DIRS=/Users/user/Project,/Users/user/Desktop,/Users/user/Downloads
```

- **First Directory Mounting**: Configuration uses first directory for Docker mount
- **Mount Configuration**: `--mount type=bind,src=<first_dir>,dst=/project`
- **Container Argument**: Pass container path as final argument (`/project`)
- **Testing Behavior**: Servers with placeholder values excluded from final configs
- **Real vs Test Environment**: Test environments without .env show servers as "PLACEHOLDERS"

### **Token Detection and Validation**
```bash
# Template for server token validation:
server_has_real_tokens() {
  local server="$1"
  local env_vars
  env_vars=$(get_expected_env_vars "$server")

  for var in $env_vars; do
    local value placeholder
    value=$(grep "^${var}=" .env 2>/dev/null | cut -d'=' -f2- | tr -d '"')
    placeholder=$(get_env_placeholder "$var")

    if [[ -z "$value" || "$value" == "$placeholder" ]]; then
      return 1
    fi
  done
  return 0
}
```

### **Container Environment Testing**
- **Test variables inside containers**: Use `docker run --env-file .env image sh -c "echo \$VAR"`
- **Validate environment visibility**: Ensure all expected variables are accessible
- **Handle mixed output gracefully**: Use `grep -o` to extract JSON from startup messages
- **Support different parse modes**: Handle both "json" and default parsing
- **Registry-based image timing**: First test of registry-based servers may take 1+ minutes for image pull (Figma lesson)

### **MCP Repository Cleanup (Build-Based Servers)**
**MANDATORY**: After building Docker images from source repositories, always clean up cloned code:

#### **Build Process Pattern**
```bash
# 1. Clone repository to temporary location
git clone https://github.com/server/repo support/docker/server-name/

# 2. Build Docker image using cloned source
docker build -t local/server-name:latest support/docker/server-name/

# 3. MANDATORY CLEANUP: Remove cloned repository
rm -rf support/docker/server-name/
```

#### **Cleanup Requirements**
- ✅ **ALWAYS remove cloned repositories**: Full `.git/` directories, source code, documentation
- ✅ **Keep only essential files**: Dockerfile, build scripts if needed for debugging
- ✅ **Verify image exists**: Confirm `docker images | grep server-name` shows built image
- ✅ **Test functionality**: Ensure built image works before cleanup
- ❌ **NEVER commit cloned repositories**: Prevents repository bloat and licensing issues

#### **Detection and Verification**
```bash
# Find any remaining cloned repositories (excluding main project)
find . -name ".git" -type d | grep -v "^\\./.git$"

# Should return empty after proper cleanup
```

#### **Emergency Recovery**
If cleanup removed necessary files:
- Built Docker images remain available even after source cleanup
- Re-clone repository if debugging or modification needed
- Use `docker history image-name` to understand image layers if issues arise

### **Debug Output Prevention (CRITICAL)**
- ❌ **NEVER leave debug trace enabled**: Check for `set -x`, `functrace`, etc.
- ❌ **NEVER output variable assignments**: Avoid `container_value=`, `image=` patterns
- ✅ **Use structured output**: Tree format with proper status indicators
- ✅ **Redirect debug info**: Send debugging to stderr or suppress entirely
- ✅ **Test JSON validity**: Always validate with `jq` before committing

### **Health Check Architecture**
- **Basic Protocol Tests**: Always work in CI (no authentication required)
- **Advanced Functionality Tests**: Only when real tokens detected
- **Graceful Degradation**: Warn about missing tokens, don't fail
- **Container Environment Verification**: Test actual Docker variable visibility
- **Mixed Output Handling**: Support servers with startup messages before JSON

### **Testing Requirements for New Servers**
When adding MCP servers, update `spec/mcp_manager_spec.sh`:

```bash
# 1. Update expected_servers in has_expected_servers()
local expected_servers="github circleci new-server"

# 2. Add environment variable tests
The output should include "NEW_SERVER_TOKEN"
The output should include "NEW_SERVER_API_KEY"

# 3. Add placeholder tests
The output should include "your_new_server_token_here"

# 4. Add Docker image tests
The output should include "vendor/new-server:latest"
```

### **JSON Structure Consistency (CORRECTED)**
**🎯 CRITICAL UPDATE**: Both Cursor and Claude Desktop use **IDENTICAL** configuration formats.

- **✅ UNIFIED FORMAT**: Both platforms use `mcpServers` wrapper structure:
```json
{
  "mcpServers": {
    "server-name": {
      "command": "docker",
      "args": ["run", "--rm", "-i", "--env-file", "path", "image"]
    }
  }
}
```

- **❌ PREVIOUS MISCONCEPTION**: Earlier assumption that Cursor used flat structure was incorrect
- **✅ CONFIRMED BY DOCUMENTATION**: Official Cursor docs show `mcpServers` wrapper requirement
- **✅ IMPLEMENTATION STATUS**: Our configuration generation has been correct all along
- **Args Array Standard**: `["run", "--rm", "-i", "--env-file", "path", "image"]`
- **No Inline Environment Variables**: Configuration should be environment-agnostic

**References**:
- Cursor MCP Docs: https://docs.cursor.com/context/model-context-protocol
- Verified: 2025-01-03 during filesystem server directory testing

### **Error Handling for MCP Servers**
- **Missing Docker**: Graceful fallback, generate configs anyway
- **Missing Tokens**: Warning messages, not errors
- **Container Failures**: Detailed error output with full Docker response
- **JSON Parsing Errors**: Strip non-JSON content before parsing
- **File Permission Issues**: Clear error messages about config file locations

### **Shell Completion Extension**
When adding new MCP servers, update `_mcp_manager`:
```bash
# Add server names to completion
case $line[1] in
  test|health|config-write)
    _arguments '*:servers:(github circleci new-server)'
    ;;
esac
```

## 📖 Documentation Maintenance Requirements

### **README.md Update Mandate**
When making significant changes to the project, you MUST update the README.md file to reflect:

**Required Updates For:**
- **New features or tools**: Add to appropriate sections, update examples
- **Script behavior changes**: Update testing strategy, CI behavior descriptions
- **Error handling improvements**: Update troubleshooting section
- **Configuration changes**: Update MCP server documentation, environment setup
- **Testing methodology changes**: Update CI testing strategy section
- **Output format changes**: Update examples and expected behavior

**Documentation Standards:**
- ✅ **Update examples** to reflect actual current behavior
- ✅ **Maintain consistency** between code behavior and documentation
- ✅ **Add new troubleshooting entries** for newly discovered issues
- ✅ **Update version numbers** and tool lists when dependencies change
- ✅ **Validate all links** and commands in documentation still work

**Process:**
1. Make code changes
2. Test functionality
3. Update README.md accordingly
4. Commit both code AND documentation changes together

**❌ Anti-Pattern**: Never commit significant functionality changes without corresponding README updates

## 🧪 Test-Driven Development (TDD) & Behavior-Driven Development (BDD) Principles

### **TDD Methodology (Red-Green-Refactor)**
**MANDATORY**: All new features MUST follow TDD cycle:

1. **🔴 RED**: Write failing tests first
2. **🟢 GREEN**: Write minimal code to make tests pass
3. **🔵 REFACTOR**: Improve code while keeping tests green

## 🎯 **Generalized MCP Server Addition Test Framework**

### **📋 MCP Server Addition Protocol (Based on Docker & Kubernetes Experience)**

When adding ANY new MCP server, follow this exact TDD-based protocol:

#### **🔴 Phase 1: RED - Comprehensive Test Writing (Before ANY Implementation)**

**1.1 Registry Integration Tests**
```bash
# Add to spec/mcp_manager_spec.sh
Describe '<server_name> server integration'
  It 'includes <server_name> server in available servers list'
    When run zsh "$PWD/mcp_manager.sh" list
    The status should be success
    The output should include "<server_name>"
  End

  It 'supports <server_type> server type parsing'
    When run zsh "$PWD/mcp_manager.sh" parse <server_name> server_type
    The status should be success
    The output should equal "<server_type>"
  End
End
```

**1.2 Configuration Generation Tests**
```bash
# Test both Cursor and Claude Desktop configurations
It 'generates <server_name> configuration for Cursor'
  zsh "$PWD/mcp_manager.sh" config-write > /dev/null 2>&1
  When run jq '.<server_name>' "$HOME/.cursor/mcp.json"
  The status should be success
  The output should include "<server_name>"
  The output should include "<expected_image>"
End

It 'generates <server_name> configuration for Claude Desktop'
  zsh "$PWD/mcp_manager.sh" config-write > /dev/null 2>&1
  When run jq '.mcpServers.<server_name>' "$HOME/Library/Application Support/Claude/claude_desktop_config.json"
  The status should be success
  The output should include "<server_name>"
End
```

**1.3 Environment Variable Tests**
```bash
# Test environment variable generation and placeholders
It 'includes <server_name> environment variables in .env_example'
  zsh "$PWD/mcp_manager.sh" config-write > /dev/null 2>&1
  When run grep "<ENV_VAR_1>\|<ENV_VAR_2>" "$PWD/.env_example"
  The status should be success
  The output should include "<ENV_VAR_1>"
  The output should include "<ENV_VAR_2>"
End

It '<server_name> environment variables include correct placeholders'
  zsh "$PWD/mcp_manager.sh" config-write > /dev/null 2>&1
  When run grep "<ENV_VAR_1>\|<ENV_VAR_2>" "$PWD/.env_example"
  The status should be success
  The output should include "<expected_placeholder_1>"
  The output should include "<expected_placeholder_2>"
End
```

**1.4 Server Type-Specific Configuration Tests**
```bash
# For api_based servers:
It '<server_name> server uses standard --env-file approach'

# For mount_based servers:
It '<server_name> server uses mount-based configuration'

# For privileged servers:
It '<server_name> server uses privileged configuration with special access'
  # Test for Docker socket, networks, volumes, etc.

# For standalone servers:
It '<server_name> server requires no external dependencies'
```

**1.5 Individual Server Testing**
```bash
It '<server_name> server can be tested individually'
  When run zsh "$PWD/mcp_manager.sh" test <server_name>
  The status should be success
  The output should include "<Server Display Name>"
End
```

**1.6 Integration Testing**
```bash
It '<server_name> integrates with existing servers without conflicts'
  When run zsh "$PWD/mcp_manager.sh" test
  The status should be success
  The output should include "<server_name>"
  The output should include "github"  # Existing server still works
End
```

#### **🟢 Phase 2: GREEN - Minimal Implementation (Make Tests Pass)**

**2.1 Registry Entry** (Always first step)
```yaml
# Add to mcp_server_registry.yml
<server_name>:
  name: "<Display Name>"
  server_type: "<api_based|mount_based|privileged|standalone>"
  description: "<Description>"
  category: "<category>"
  source:
    type: registry  # OR: type: build for source-based servers
    image: "<image_url>"  # OR: repository: "<github_url>" + image: "local/<name>:latest"
    # For build type, add: build_context: "."
  environment_variables:  # If applicable
    - "<ENV_VAR_1>"
    - "<ENV_VAR_2>"
  # For privileged servers, add:
  volumes:  # Use exact format - functions depend on this structure
    - "<host_path>:<container_path>:<options>"
  networks:
    - "<network_name>"
```

**2.2 Environment Variable Placeholders** (If new variables)
```bash
# Add to get_env_placeholder() function in mcp_manager.sh
"<ENV_VAR_1>")
  echo "<specific_placeholder_value>"
  ;;
"<ENV_VAR_2>")
  echo "<specific_placeholder_value>"
  ;;
```

**2.3 Server Type Extension** (If new server type needed)
```bash
# Extend all case statements that handle server_type:
# - Configuration generation (both Cursor and Claude)
# - Advanced functionality testing
# - Token detection logic
# - Preview generation

"<new_server_type>")
  <type_specific_handling>
  ;;
```

**2.4 Function Ordering Verification**
```bash
# CRITICAL: Ensure all helper functions are defined BEFORE use
# Check function call order:
1. Helper functions (get_*, server_*, etc.)
2. Test functions (test_*_advanced_functionality)
3. Configuration functions (generate_*, write_*)
4. Main command interface
```

#### **🔵 Phase 3: REFACTOR - Optimization & Documentation**

**3.1 Shell Completion Validation**
```bash
# Verify completion works automatically
awk '/^  [a-z].*:$/ { gsub(/:/, ""); gsub(/^  /, ""); print }' mcp_server_registry.yml
# Should include new server

# Test completion manually
./mcp_manager.sh test <TAB>  # Should show new server
```

**3.2 Documentation Updates**
- Update README.md with new server capabilities
- Add troubleshooting entries for server-specific issues
- Update examples and use cases

**3.3 Integration Verification**
```bash
# Full integration test suite
./mcp_manager.sh test                    # All servers work
./mcp_manager.sh config-write            # All configs generate
./mcp_manager.sh inspect                 # Inspector integration
```

### **🏗️ Server Type Classification Quick Reference**

#### **🎯 Simple Decision Tree (Figma-Validated)**
**Question 1**: Does it need API tokens/keys for external services?
- **YES** → `api_based` (GitHub, CircleCI, Figma, Slack, Linear)

**Question 2**: Does it need to access local files/directories?
- **YES** → `mount_based` (Filesystem, Local Dev Tools)

**Question 3**: Does it need Docker socket, system networks, or special volumes?
- **YES** → `privileged` (Docker, Kubernetes, System Tools)

**Question 4**: No external dependencies at all?
- **YES** → `standalone` (Inspector, Utilities, Converters)

**🚨 Anti-Pattern**: Don't add networks/volumes unless the server truly requires system-level access.

#### **📋 Detailed Classifications**

**api_based**: GitHub, CircleCI, Figma, Linear, Slack
- Uses `--env-file` for API tokens
- Standard Docker configuration
- Token validation in advanced tests

**mount_based**: Filesystem, Local Development Tools
- Uses `--mount` for directory access
- Container path arguments
- File system operation tests

**privileged**: Docker, Kubernetes, System Tools
- Requires special volumes (Docker socket, kubeconfig) and/or network access
- Uses `volumes:` and `networks:` in registry configuration
- System-level validation tests
- May need entrypoint overrides for STDIO mode
- Critical: YAML structure must be `volumes:` and `networks:` directly under server entry

**standalone**: Inspector, Utilities, Converters
- No external dependencies
- No authentication required
- Self-contained functionality tests

### **⚠️ Common Pitfalls to Avoid (Learned from Docker, Kubernetes & Figma Experience)**

1. **Function Ordering**: Always define helper functions BEFORE they're called
2. **Missing Test Categories**: Every server needs basic + advanced + integration tests
3. **Hardcoded Patterns**: Use server_type dispatch, never hardcode server names
4. **Configuration Gaps**: Test BOTH Cursor AND Claude Desktop configs
5. **Shell Completion**: Verify tab completion works after addition
6. **Environment Variables**: Always test placeholder generation
7. **Interactive Commands**: Use non-interactive flags (`-f`, `--yes`, `cat >`)
8. **Documentation Sync**: Update README.md with every functional change
9. **Docker Entrypoint Issues**: Check if images have hardcoded entrypoints (like SSE mode) and override for STDIO testing
10. **Build Dependencies**: Ensure `setup` command builds images before `test` command is run
11. **Privileged Configuration Structure**: Use correct YAML structure for volumes/networks - functions parse these directly
12. **Container Path Mapping**: Environment variables like KUBECONFIG need container paths, not host paths
13. **Test Environment Differences**: Manual testing may work while automated tests fail due to environment isolation
14. **Server Type Over-engineering**: Don't overcomplicate server types - use simple patterns unless truly necessary (Figma lesson: API token services are `api_based`, not `privileged`)

### **✅ Definition of Done for MCP Server Addition**

A new MCP server is complete when:
- [ ] All tests pass (8+ tests minimum covering all aspects)
- [ ] Configuration generated for both Cursor and Claude Desktop
- [ ] Environment variables have specific placeholders (not generic)
- [ ] Shell completion includes new server
- [ ] Integration with existing servers verified
- [ ] Documentation updated
- [ ] Function ordering verified (no "command not found" errors)
- [ ] Server type classification working correctly
- [ ] Manual testing completed with real server functionality

### **📋 Copy-Paste Template for New MCP Server Addition**

```bash
# 🔴 RED: Add these tests to spec/mcp_manager_spec.sh FIRST
Describe '[SERVER_NAME] server integration'
  It 'includes [SERVER_NAME] server in available servers list'
    When run zsh "$PWD/mcp_manager.sh" list
    The status should be success
    The output should include "[SERVER_NAME]"
  End

  It 'generates [SERVER_NAME] configuration for Cursor'
    zsh "$PWD/mcp_manager.sh" config-write > /dev/null 2>&1
    When run jq '.[SERVER_NAME]' "$HOME/.cursor/mcp.json"
    The status should be success
    The output should include "[SERVER_NAME]"
    The output should include "[EXPECTED_IMAGE]"
  End

  It 'generates [SERVER_NAME] configuration for Claude Desktop'
    zsh "$PWD/mcp_manager.sh" config-write > /dev/null 2>&1
    When run jq '.mcpServers.[SERVER_NAME]' "$HOME/Library/Application Support/Claude/claude_desktop_config.json"
    The status should be success
    The output should include "[SERVER_NAME]"
  End

  It 'includes [SERVER_NAME] environment variables in .env_example'
    zsh "$PWD/mcp_manager.sh" config-write > /dev/null 2>&1
    When run grep "[ENV_VAR_1]\\|[ENV_VAR_2]" "$PWD/.env_example"
    The status should be success
    The output should include "[ENV_VAR_1]"
    The output should include "[ENV_VAR_2]"
  End

  It '[SERVER_NAME] environment variables include correct placeholders'
    zsh "$PWD/mcp_manager.sh" config-write > /dev/null 2>&1
    When run grep "[ENV_VAR_1]\\|[ENV_VAR_2]" "$PWD/.env_example"
    The status should be success
    The output should include "[EXPECTED_PLACEHOLDER_1]"
    The output should include "[EXPECTED_PLACEHOLDER_2]"
  End

  It '[SERVER_NAME] server uses [SERVER_TYPE] configuration'
    zsh "$PWD/mcp_manager.sh" config-write > /dev/null 2>&1
    When run jq '.[SERVER_NAME].args[]' "$HOME/.cursor/mcp.json"
    The status should be success
    # Add server-type specific assertions here
  End

  It '[SERVER_NAME] server can be tested individually'
    When run zsh "$PWD/mcp_manager.sh" test [SERVER_NAME]
    The status should be success
    The output should include "[DISPLAY_NAME]"
  End

  It '[SERVER_NAME] server supports [SERVER_TYPE] server type'
    When run zsh "$PWD/mcp_manager.sh" parse [SERVER_NAME] server_type
    The status should be success
    The output should equal "[SERVER_TYPE]"
  End
End

# 🟢 GREEN: Run tests (should fail), then implement minimal code to pass
# 1. Add registry entry
# 2. Add environment variable placeholders (if needed)
# 3. Extend server type handling (if new type)
# 4. Verify function ordering

# 🔵 REFACTOR: Optimize, document, validate integration
```

**Usage Instructions:**
1. Replace `[SERVER_NAME]` with actual server ID (e.g., `slack`, `linear`)
2. Replace `[DISPLAY_NAME]` with human-readable name (e.g., `Slack MCP Server`)
3. Replace `[SERVER_TYPE]` with classification (e.g., `api_based`, `privileged`)
4. Replace `[ENV_VAR_1]`, `[ENV_VAR_2]` with actual environment variables
5. Replace `[EXPECTED_PLACEHOLDER_1]` with specific placeholder values
6. Replace `[EXPECTED_IMAGE]` with Docker image URL

**Estimated Time per Server**: 15-30 minutes following this protocol vs. hours of debugging without it.

### **Test-First Development Rules**
- ❌ **NEVER write production code without a failing test first**
- ✅ **ALWAYS start with the test that describes the expected behavior**
- ✅ **Write the smallest possible test that fails**
- ✅ **Only write enough production code to make the test pass**

### **ShellSpec Testing Standards**

#### **Test Organization**
```bash
# File structure for tests
spec/
├── spec_helper.sh          # Shared test utilities
├── mcp_manager_spec.sh     # Core MCP manager functionality
├── mcp_inspector_spec.sh   # New: Inspector functionality tests
└── integration_spec.sh     # End-to-end integration tests
```

#### **Test Writing Patterns**
```bash
# Use descriptive test names that explain behavior
Describe 'mcp_manager.sh inspect command'
  Describe 'when inspecting all servers'
    Context 'with no running containers'
      It 'should display "No MCP servers currently running"'
        When run ./mcp_manager.sh inspect
        The output should include "No MCP servers currently running"
        The status should be success
      End
    End

    Context 'with running containers'
      BeforeEach 'start_test_containers'
      AfterEach 'cleanup_test_containers'

      It 'should discover and list running servers'
        When run ./mcp_manager.sh inspect
        The output should include "Running MCP servers"
        The output should include "github"
        The status should be success
      End
    End
  End
End
```

#### **Test Categories by Scope**
1. **Unit Tests**: Test individual functions in isolation
2. **Integration Tests**: Test component interactions
3. **System Tests**: Test complete workflows end-to-end
4. **CI Tests**: Tests that run in CI environment (no Docker dependencies)

### **BDD Behavior Specification**

#### **Feature Definition Template**
```bash
# Every new feature starts with behavior specification
Describe 'MCP Inspector Feature'
  Describe 'inspect command basic functionality'
    It 'should show help when no arguments provided'
    It 'should discover running MCP server containers'
    It 'should validate environment variables'
    It 'should test server connectivity'
  End

  Describe 'inspect command advanced features'
    It 'should launch web UI on --ui flag'
    It 'should validate client configurations'
    It 'should run in CI-friendly mode'
    It 'should debug specific servers with --debug'
  End
End
```

#### **Given-When-Then Structure**
```bash
# Use Context/Setup for Given, When for actions, assertions for Then
Context 'Given a configured MCP server environment'
  Setup 'configure_test_environment'

  It 'When inspecting a specific server, Then should show detailed information'
    When run ./mcp_manager.sh inspect github
    The output should include "=== MCP Server Inspection: github ==="
    The output should include "[SERVER] GitHub MCP Server"
    The status should be success
  End
End
```

### **Test Implementation Workflow**

#### **For New Features (Inspector Example)**
```bash
# 1. Define the behavior first (RED)
It 'should provide UI access to MCP inspector'
  When run ./mcp_manager.sh inspect --ui
  The output should include "Inspector UI started"
  The output should include "http://localhost:6274"
  The status should be success
End

# 2. Run test (should fail)
shellspec spec/mcp_inspector_spec.sh

# 3. Implement minimal code to pass (GREEN)
# Add basic --ui handling to handle_inspect_command()

# 4. Run test (should pass)
shellspec spec/mcp_inspector_spec.sh

# 5. Refactor and improve (REFACTOR)
# Enhance error handling, improve output format
```

#### **Test Environment Setup Standards**
```bash
# Use consistent test helper patterns
setup_test_environment() {
  export TEST_HOME="$PWD/test_home"
  export CI=false
  mkdir -p "$TEST_HOME/.cursor"
  mkdir -p "$TEST_HOME/Library/Application Support/Claude"
}

cleanup_test_environment() {
  rm -rf "$TEST_HOME"
  docker container prune -f 2>/dev/null || true
}

mock_docker_commands() {
  # Create mock functions for Docker in test environment
  docker() {
    case "$1" in
      "ps") echo "CONTAINER ID   IMAGE                     NAMES"
            echo "123abc         mcp/github:latest        test-github" ;;
      "images") echo "mcp/github   latest   123   2 hours ago   100MB" ;;
    esac
  }
}
```

### **Test Coverage Requirements**

#### **Mandatory Test Coverage**
- ✅ **All new commands must have tests**
- ✅ **All error conditions must be tested**
- ✅ **Both CI and local environments must be tested**
- ✅ **Edge cases and error handling must be covered**

#### **Test Scenarios Matrix**
```bash
# Every new feature needs tests for:
#
# Environment Matrix:
# - Local development (Docker available)
# - CI environment (CI=true, limited Docker)
# - Missing dependencies (no Docker, no tools)
#
# Input Matrix:
# - Valid inputs
# - Invalid inputs
# - Missing inputs
# - Edge case inputs
#
# State Matrix:
# - Clean environment
# - Existing configurations
# - Partially configured state
# - Error states
```

### **Test Quality Standards**

#### **Test Readability Rules**
- ✅ **Test names should read like documentation**
- ✅ **Use descriptive Context and Describe blocks**
- ✅ **One assertion per test when possible**
- ✅ **Setup and teardown should be clear and isolated**

#### **Test Reliability Rules**
- ✅ **Tests must be deterministic (no flaky tests)**
- ✅ **Tests must be isolated (no dependencies between tests)**
- ✅ **Tests must clean up after themselves**
- ✅ **Tests must work in both CI and local environments**

### **Debugging Test Failures**

#### **Test Debugging Commands**
```bash
# Run specific test with verbose output
shellspec --format documentation spec/mcp_inspector_spec.sh

# Run single test case
shellspec --example "should discover running servers" spec/mcp_inspector_spec.sh

# Debug test with trace
shellspec --trace spec/mcp_inspector_spec.sh
```

#### **Common Test Failure Patterns**
- **Environment not isolated**: Tests affecting each other
- **Missing mocks**: Tests failing due to external dependencies
- **Race conditions**: Tests depending on timing
- **Path issues**: Tests not finding required files

### **Test-Driven Refactoring**

#### **Safe Refactoring Process**
1. ✅ **Ensure all tests pass before refactoring**
2. ✅ **Add tests for edge cases if missing**
3. ✅ **Refactor incrementally**
4. ✅ **Run tests after each small change**
5. ✅ **Never skip tests because "it's just refactoring"**

### **TDD Anti-Patterns to Avoid**

#### **Development Anti-Patterns**
- ❌ **Writing production code before writing tests**
- ❌ **Writing tests that always pass (testing implementation, not behavior)**
- ❌ **Skipping the RED phase (not seeing tests fail first)**
- ❌ **Making multiple changes before running tests**

#### **Test Anti-Patterns**
- ❌ **Tests that depend on external services**
- ❌ **Tests that require manual setup**
- ❌ **Tests that test implementation details instead of behavior**
- ❌ **Giant tests that test multiple behaviors**

### **Integration with Existing Workflow**

#### **Updated Testing Pipeline**
```bash
# Enhanced testing pipeline for TDD workflow:
1. Write failing test first
2. zsh -n setup.sh && zsh -n verify_setup.sh  # Syntax check
3. shellspec spec/                            # Run all tests
4. Write minimal code to pass the test
5. shellspec spec/                            # Verify test passes
6. ./verify_setup.sh > /dev/null              # Integration test
7. pre-commit run --all-files                 # Code quality
8. MANDATORY CLEANUP PROTOCOL                 # See 🧹 Mandatory Cleanup Protocol
9. git add . && git commit -m "descriptive message"
10. git push origin main
```

#### **CI Integration**
- ✅ **All tests must pass in CI environment**
- ✅ **Tests should gracefully handle CI limitations (no Docker)**
- ✅ **Test results should be clearly reported**
- ✅ **Failed tests should provide actionable error messages**

## 🛡️ Major Change Approval Protocol

### **When to Require User Confirmation**
Any change that falls into these categories **MUST** get explicit user approval before implementation:

#### **Structural Changes**
- ❌ **NEVER** reorganize directory structure without approval
- ❌ **NEVER** move files to new locations without confirmation
- ❌ **NEVER** delete files or directories (even temp files) without asking
- ❌ **NEVER** rename core scripts or configuration files

#### **Destructive Operations**
- ❌ **NEVER** use `rm`, `mv`, or `git rm` commands without explicit permission
- ❌ **ALWAYS** use `rm -f` or `rm -rf` to avoid interactive prompts when removal is approved
- ❌ **NEVER** remove or modify existing documentation sections
- ❌ **NEVER** change established workflows or testing pipelines
- ❌ **NEVER** modify configuration files that affect user environment

#### **Required Approval Process**
When a major change is needed:

1. **🛑 STOP and present proposal**: "I recommend the following changes: [detailed list]"
2. **📋 Explain rationale**: Why these changes are needed and what benefits they provide
3. **⚠️ Highlight risks**: What could go wrong or what will be affected
4. **🤔 Wait for approval**: "Do you approve this approach?" or "Would you prefer a different solution?"
5. **✅ Only proceed after explicit confirmation**: User says "yes," "proceed," or "approved"

#### **Example Proposal Format**
```
## 📋 PROPOSAL: Directory Restructure

**Problem**: Supporting files are cluttering the root directory

**Proposed Changes**:
- Create support/ directory structure
- Move Dockerfile.mcp-inspector → support/docker/mcp-inspector/
- Move _mcp_manager → support/completions/
- Remove outdated .envrc_example

**Benefits**: Cleaner root, better organization, scalable for future MCP servers

**Risks**: Need to update references in scripts, potential disruption

**Question**: Do you approve this directory restructure approach?
```

### **Minor Changes That Don't Need Approval**
- ✅ Bug fixes in existing functionality
- ✅ Code formatting and style improvements
- ✅ Adding tests for existing features
- ✅ Documentation clarifications and corrections
- ✅ Performance optimizations that don't change behavior

### **When in Doubt**
- **Always err on the side of asking for permission**
- **Present proposals before implementation**
- **Respect the user's project ownership**
- **Remember: it's easier to get approval than to fix mistakes**

## 🧹 Mandatory Cleanup Protocol

### **When Cleanup is Required**
Cleanup is **MANDATORY** at the end of every major development step, including:

#### **Major Development Milestones**
- ✅ **Feature Implementation**: After adding new commands, functions, or capabilities
- ✅ **Bug Fixes**: After resolving issues that touched multiple files
- ✅ **Refactoring**: After code reorganization or optimization
- ✅ **Testing**: After adding new test suites or major test changes
- ✅ **Documentation**: After significant README or documentation updates
- ✅ **Configuration Changes**: After modifying workflows, CI, or project structure

### **Mandatory Cleanup Checklist**

#### **🗂️ File Organization**
- ✅ **Remove temporary files**: Delete `*.tmp`, `test_home/`, build artifacts
  - **Note**: ShellSpec's `AfterAll()` cleanup may not execute reliably, so manual cleanup of `test_home/` is often required
- ✅ **Clean up cloned MCP server repositories**: Remove full git repositories after Docker image builds (keep only necessary files like Dockerfile, but remove .git/, source code, docs, etc.)
- ✅ **Remove backup files**: Delete `.env.backup`, `*.bak`, and other temporary backup files
- ✅ **Organize supporting files**: Ensure proper placement in `support/` structure
- ✅ **Check for outdated files**: Remove deprecated configurations or examples
- ✅ **Validate file permissions**: Ensure executable files have correct permissions

#### **📝 Documentation Sync**
- ✅ **Update README.md**: Reflect any new functionality or changed workflows
- ✅ **Update help text**: Ensure `./mcp_manager.sh help` matches implementation
- ✅ **Update completion files**: Ensure shell completion matches available commands
- ✅ **Update .cursorrules**: Document new patterns or learned best practices

#### **🧪 Quality Assurance**
- ✅ **Run full test suite**: `shellspec spec/` - all tests must pass
- ✅ **Syntax validation**: `zsh -n setup.sh && zsh -n verify_setup.sh`
- ✅ **Integration test**: `./verify_setup.sh > /dev/null` (expect success or known issues)
- ✅ **Pre-commit checks**: `pre-commit run --all-files` - all checks must pass

#### **🔧 Code Quality**
- ✅ **Remove debug output**: No `set -x`, `echo "debug:"`, or variable assignments in output
- ✅ **Update path references**: Ensure all file paths point to correct locations
- ✅ **Validate configurations**: Test that generated configs work correctly
- ✅ **Check for unused functions**: Remove or document any unused code
- ✅ **CI output validation**: Ensure CI pipelines have clean output without shell initialization errors

#### **📦 Git Hygiene**
- ✅ **Stage all changes**: `git add -A` (after verifying no sensitive files)
- ✅ **Descriptive commit**: Use format `<type>: <description> - <details>`
- ✅ **Push to remote**: `git push origin main` after successful commit
- ✅ **Verify CI**: Ensure remote builds pass (if applicable)

### **Cleanup Enforcement Rules**

#### **🛑 Mandatory Cleanup Process**
At the end of every major development step, you MUST:

1. **📋 Announce Cleanup**: "Beginning mandatory cleanup for [feature/fix/refactor]"
2. **🧹 Execute Checklist**: Work through entire cleanup checklist systematically
3. **✅ Verify Results**: Confirm all checklist items completed successfully
4. **📝 Document Status**: Report cleanup completion and any remaining items
5. **🚀 Final Commit**: Only proceed after cleanup is 100% complete

#### **❌ No Exceptions**
- **Never skip cleanup** because "it's just a small change"
- **Never defer cleanup** to "next time" or "later"
- **Never commit** without completing the full cleanup checklist
- **Never push** without verifying all tests pass

## 🚫 Interactive Command Anti-Patterns

### **CRITICAL: Always Use Non-Interactive Flags**
When using file operations in `run_terminal_cmd`, **ALWAYS** use non-interactive flags to prevent hanging on confirmations:

#### **Required Non-Interactive Patterns**
```bash
# ✅ CORRECT - Non-interactive file operations
cp -f source destination     # Force overwrite without asking
mv source destination         # Move (usually non-interactive by default)
rm -f file                    # Force delete without asking
rm -rf directory              # Force recursive delete without asking
git add -A                    # Add all changes without interaction

# ❌ NEVER USE - Interactive commands that will hang
cp source destination         # Will ask "overwrite?" and hang
rm file                       # Will ask confirmation for protected files
rm -r directory              # Will ask before removing directory
```

#### **Shell Command Safety Rules**
- ✅ **ALWAYS use `-f` flag** for `cp`, `rm` operations
- ✅ **Test commands locally first** if unsure about interactivity
- ✅ **Use `|| true`** for commands that might fail safely
- ❌ **NEVER assume commands are non-interactive** without verification
- ❌ **NEVER use interactive shells** (`zsh -i`, `bash -i`) in automation

#### **Common Interactive Traps to Avoid**
```bash
# These WILL hang and must be avoided:
cp .env_example .env                    # ❌ Asks overwrite confirmation
rm important_file                       # ❌ Asks confirmation for write-protected
mv file existing_file                   # ❌ Asks overwrite confirmation
git reset --hard                        # ❌ May ask confirmation in some setups
npm install                             # ❌ May ask for audit fix confirmations

# Use these safe alternatives instead:
cat source > destination               # ✅ Shell redirection - completely non-interactive
cp -f .env_example .env                 # ✅ Forces overwrite (but may still prompt on some systems)
rm -f important_file                    # ✅ Forces deletion
mv file existing_file 2>/dev/null || true  # ✅ Suppress errors if needed
git reset --hard HEAD                   # ✅ More specific, less likely to prompt
npm install --yes                       # ✅ Auto-confirm prompts
```

#### **Emergency Recovery Protocol**
If a command hangs due to interactive prompt:
1. **Recognize the pattern**: Command stops responding, cursor shows waiting
2. **Document the fix**: Note what flag was missing (`-f`, `--yes`, etc.)
3. **Update this rule**: Add the specific case to prevent recurrence
4. **Use force flags**: Always prefer automation over interaction

#### **🔄 Cleanup Failure Recovery**
If cleanup reveals issues:

1. **🛑 Stop development**: Do not proceed with new features
2. **🔧 Fix issues**: Address any test failures or problems discovered
3. **🧹 Re-run cleanup**: Complete the full checklist again
4. **✅ Verify success**: Ensure all issues are resolved
5. **📝 Document lessons**: Update .cursorrules with new patterns if needed

### **Cleanup Templates**

#### **Standard Cleanup Announcement**
```
## 🧹 **Mandatory Cleanup: [Feature Name]**

### **Development Completed**
- ✅ [List major changes made]
- ✅ [Key functionality added/modified]

### **Cleanup Checklist**
- [ ] Remove temporary files
- [ ] Organize supporting files
- [ ] Update documentation
- [ ] Run full test suite
- [ ] Validate syntax and integration
- [ ] Check code quality
- [ ] Commit and push changes

### **Status**: [IN PROGRESS/COMPLETED]
```

#### **Cleanup Completion Report**
```
## ✅ **Cleanup Complete: [Feature Name]**

### **Files Organized**
- ✅ Temporary files removed: [list]
- ✅ Supporting files organized: [moved files]
- ✅ Outdated files removed: [list]

### **Quality Verification**
- ✅ Tests passing: [X/X]
- ✅ Syntax validation: PASS
- ✅ Integration test: PASS
- ✅ Pre-commit checks: PASS

### **Documentation Updated**
- ✅ README.md: [changes made]
- ✅ Help text: [updated sections]
- ✅ Completion files: [new commands added]

### **Ready for Next Development Phase** 🚀
```

## 🤖 CI Environment Standards

### **Clean Output Requirements**
- ✅ **No shell initialization errors**: Suppress `.zshrc` loading errors in CI with `2>/dev/null || true`
- ✅ **No debug variable assignments**: Filter out `container_value=`, `server_count=`, etc.
- ✅ **No command not found errors**: Handle missing tools gracefully in CI environments
- ✅ **Structured output from line 1**: Ensure first line starts with section headers (`===`)

### **CI-Specific Error Handling**
- ✅ **Shell setup errors**: Suppress plugin/completion failures that don't affect functionality
- ✅ **Missing tool warnings**: Distinguish between critical failures and CI environment limitations
- ✅ **Container unavailability**: Graceful degradation when Docker/OrbStack not available
- ✅ **Test environment isolation**: Ensure CI tests don't depend on local development setup

### **CI Output Validation Tests**
- ✅ **Add `spec/ci_output_spec.sh`**: Dedicated tests for clean CI output
- ✅ **Test shell initialization**: Verify no stray error messages appear
- ✅ **Test debug output filtering**: Ensure all debug variables are filtered
- ✅ **Test structured output**: Verify consistent formatting across environments

## 📝 Cursor Rules Maintenance Protocol

### **🎯 Rule Update Methodology**
After each major development iteration (new feature, server addition, significant debugging), systematically update `.cursorrules`:

#### **1. Capture Key Learnings**
- **What worked well?** → Add to best practices sections
- **What caused issues?** → Add to anti-patterns and pitfalls
- **What was unexpected?** → Document edge cases and special handling
- **What would help next time?** → Add to templates and checklists

#### **2. Focused Rule Updates**
- ✅ **Update relevant sections only** - Don't rewrite everything
- ✅ **Add specific examples** from the actual work completed
- ✅ **Include exact code patterns** that were successful
- ✅ **Document exact error patterns** to avoid repeating
- ✅ **Update templates** with new required steps discovered

#### **3. Rule Quality Standards**
- **Specific over Generic**: "Use `--entrypoint /app/server` for Kubernetes server" vs "Override entrypoints"
- **Actionable over Descriptive**: "Add volumes: under server entry" vs "Handle volumes properly"
- **Example-Rich**: Include exact YAML, commands, and code snippets
- **Categorized**: Place updates in logical sections for easy reference

#### **4. Validation Process**
- ✅ **Test new patterns** on next iteration to verify they work
- ✅ **Remove outdated rules** that no longer apply
- ✅ **Consolidate duplicate guidance** into single, clear statements
- ✅ **Maintain rule hierarchy** from general principles to specific examples

### **🔄 Iterative Improvement Examples**

**Before (Generic):**
```
- Handle Docker containers properly
- Test thoroughly
- Update configuration
```

**After (Specific, from Kubernetes experience):**
```
- Override entrypoints with --entrypoint for servers that default to SSE mode
- Map host environment variables to container paths (KUBECONFIG=/home/.kube/config)
- Use volumes: and networks: directly under server entry, not nested under privileged_configuration
- Build images with setup command before testing build-based servers
```

## 🔬 **Ultimate Integration Testing: MCP-to-Cursor Protocol Validation**

### **🎯 The Methodology**
**Concept**: Use Cursor's native MCP integration to directly communicate with our configured MCP servers. If the AI model can successfully call tools and access resources from our servers, it proves the entire configuration pipeline works end-to-end.

### **📋 Implementation Blueprint**

#### **Step 1: Configuration Generation & Setup**
```bash
# Generate Cursor MCP configuration
./mcp_manager.sh config-write cursor

# Ensure all required servers are built (for build-based servers)
./mcp_manager.sh setup circleci    # Build source-based servers
./mcp_manager.sh setup kubernetes  # Build source-based servers

# Verify configuration exists
cat "$HOME/.cursor/mcp.json" | jq '.'
```

#### **Step 2: Environment Configuration**
```bash
# Copy environment template and add real tokens
cp .env_example .env

# Add real API tokens for servers requiring authentication:
# - GITHUB_TOKEN (for GitHub server)
# - CIRCLECI_TOKEN (for CircleCI server)
# - Update KUBECONFIG path for Kubernetes server
# - Update FILESYSTEM_ALLOWED_DIRS for filesystem server
```

#### **Step 3: Cursor MCP Integration Test**
```markdown
**Test Protocol**: Ask the AI model to:

1. **List Available Tools**: "What MCP tools do you have available?"
2. **Test Each Server Type**:
   - **GitHub**: "Can you list repositories in my GitHub account?"
   - **CircleCI**: "Can you show my CircleCI projects?"
   - **Filesystem**: "Can you list files in the MacbookSetup directory?"
   - **Docker**: "Can you list Docker containers?"
   - **Kubernetes**: "Can you list Kubernetes namespaces?"
   - **Inspector**: "Can you use the MCP inspector tools?"

3. **Validate Responses**: Verify the AI can actually execute these commands and return real data
```

#### **Step 4: Success Criteria**
- ✅ **Tool Discovery**: AI model can see and list all configured MCP server tools
- ✅ **Authentication**: Servers requiring tokens work with real credentials
- ✅ **Data Retrieval**: AI can fetch actual data from external services (GitHub repos, K8s resources, etc.)
- ✅ **Error Handling**: Graceful handling of missing credentials or unavailable services
- ✅ **Multiple Server Types**: All server types (api_based, mount_based, privileged, standalone) work

### **🔍 Troubleshooting Guide**

**Problem**: "No MCP tools available"
- **Solution**: Check if `~/.cursor/mcp.json` exists and is valid JSON
- **Verification**: Restart Cursor after configuration changes

**Problem**: "Authentication failed for [server]"
- **Solution**: Verify `.env` file has real tokens, not placeholders
- **Verification**: Run `./mcp_manager.sh test [server]` manually

**Problem**: "Docker image not found"
- **Solution**: Run `./mcp_manager.sh setup [server]` to build missing images
- **Verification**: Check `docker images` for required images

**Problem**: "Mount/volume access denied"
- **Solution**: Verify file paths exist and Docker has permission
- **Verification**: Test mount commands manually with `docker run`

### **💡 Benefits of This Approach**

1. **End-to-End Validation**: Tests the complete pipeline from registry → configuration → AI integration
2. **Claude Desktop Compatibility**: If Cursor MCP works, Claude Desktop will work (same protocol)
3. **Real-World Testing**: Uses actual API calls and data, not mocked responses
4. **Immediate Feedback**: Quickly identifies configuration issues vs waiting for external testing
5. **Documentation Validation**: Proves our templates and documentation are correct

### **📝 Integration Test Template**

```markdown
## MCP Integration Test Results

**Date**: [Current Date]
**Servers Tested**: [List servers]

### Test Results:
- [ ] GitHub MCP Server: Tools available ✅/❌ | Authentication ✅/❌ | Data retrieval ✅/❌
- [ ] CircleCI MCP Server: Tools available ✅/❌ | Authentication ✅/❌ | Data retrieval ✅/❌
- [ ] Filesystem MCP Server: Tools available ✅/❌ | Mount access ✅/❌ | File operations ✅/❌
- [ ] Docker MCP Server: Tools available ✅/❌ | Socket access ✅/❌ | Container operations ✅/❌
- [ ] Kubernetes MCP Server: Tools available ✅/❌ | Cluster access ✅/❌ | Resource operations ✅/❌
- [ ] Inspector MCP Server: Tools available ✅/❌ | Protocol access ✅/❌ | Debug operations ✅/❌

### Notes:
[Any issues encountered, workarounds used, or observations]

### Conclusion:
✅ **PASSED**: All servers functional - ready for Claude Desktop integration
❌ **FAILED**: [List issues] - requires fixes before production use
```

### **🔄 When to Run Integration Tests**
- After adding any new MCP server
- After modifying configuration generation logic
- Before committing major changes to the MCP system
- When troubleshooting Claude Desktop integration issues
- As part of the final validation in TDD cycles

### **🆕 Latest Update: Figma MCP Server Implementation & SCT Methodology Validation (Current Iteration)**

**Key Learnings Captured:**
1. **🎯 Server Type Over-engineering Anti-Pattern (Figma lesson)**:
   - Don't overcomplicate server classifications - simple patterns usually work
   - API token services are `api_based`, not `privileged` (Figma, GitHub, CircleCI pattern)
   - Added simple decision tree to prevent classification confusion
   - Avoid adding networks/volumes unless truly required for system-level access

2. **🚀 SCT (Simple Continuous Testing) Methodology Success**:
   - 5-change rule with fast/full test tiers proved effective for rapid iteration
   - Small, testable changes caught issues early and maintained stability
   - End-to-end validation through Cursor's MCP integration confirmed entire pipeline works
   - Keep automation tooling simple to avoid complexity-induced hangs

3. **🐳 Registry-Based Server Timing Considerations**:
   - First-time Docker image pulls for registry-based servers can take 1+ minutes
   - Once cached, subsequent tests are fast (< 10 seconds)
   - Plan for initial setup time in test expectations

4. **Previous Learnings (Kubernetes & Filesystem)**:
   - **Build-from-Source Pattern**: When pre-built images don't exist, use `type: build` with repository URL
   - **Entrypoint Override Required**: Some servers default to SSE mode, need `--entrypoint` override for STDIO
   - **Container Path Mapping**: Host paths → Container paths for environment variables
   - **Configuration Format**: Both Cursor and Claude Desktop use identical `mcpServers` wrapper format

**Rules Updated:**
- Added Server Type Over-engineering to Common Pitfalls (#14)
- Added Simple Decision Tree for server type classification
- Enhanced Container Environment Testing with registry timing considerations
- Validated SCT methodology through successful Figma implementation
- Confirmed Ultimate Integration Testing approach with real Figma design analysis

## 🔄 Continuous Improvement
- Document successful patterns in this file using the Rule Update Methodology above
- Update rules based on what works in practice, with specific examples
- Keep the "minimal complexity" principle as the north star
- Review and refine these rules after major changes using focused updates
- **Always maintain README.md alongside code changes**
- **Continuously improve test coverage and quality**
- **Refactor tests alongside production code**

## 🧪 ShellSpec Formatting Best Practices

- Indent each `Describe`, `Context`, and `It` block by two spaces per nesting level.
- Align all `End` statements with their corresponding block openings.
- No auto-formatter (like `shfmt`) understands ShellSpec blocks; manual or custom-script formatting is required for BDD block structure.
- For reference and examples, see the official ShellSpec repository: https://github.com/shellspec/shellspec
